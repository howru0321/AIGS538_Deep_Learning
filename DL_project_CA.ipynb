{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howru0321/AIGS538_Deep_Learning/blob/main/DL_project_CA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCXOlGaEpUwy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data as data_utils\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "\n",
        "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "base_dir = f'CA/{current_time}'\n",
        "\n",
        "num_epochs=1000\n",
        "num_save=100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess():\n",
        "  def __init__(self, train_data, test_data):\n",
        "    self.train_data = train_data\n",
        "    self.test_data = test_data\n",
        "\n",
        "    self.features = train_data.columns\n",
        "    self.in_features = [col for col in train_data.columns if train_data[col].dtype == float and col not in ('median_house_value')]\n",
        "    self.out_features = ['median_house_value']\n",
        "\n",
        "    mean, std = self.get_features_mean_std(train_data)\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def get_features_mean_std(self, data):\n",
        "    return data[self.features].mean(), data[self.features].std()\n",
        "\n",
        "  def get_train_input_output(self):\n",
        "    normalized = (self.train_data[self.features]-self.mean)/self.std\n",
        "    input = torch.Tensor(normalized[self.in_features].values)\n",
        "    output = torch.Tensor(normalized[self.out_features].values)\n",
        "    return [input, output]\n",
        "\n",
        "  def get_test_input_output(self):\n",
        "    normalized = (self.test_data[self.features]-self.mean)/self.std\n",
        "    input = torch.Tensor(normalized[self.in_features].values)\n",
        "    output = torch.Tensor(normalized[self.out_features].values)\n",
        "    return [input, output]\n",
        "\n",
        "# get data\n",
        "train_data = pd.read_csv('./sample_data/california_housing_train.csv')\n",
        "test_data = pd.read_csv('./sample_data/california_housing_test.csv')\n",
        "\n",
        "preprocessor = Preprocess(train_data=train_data, test_data=test_data)\n",
        "train_dataset = preprocessor.get_train_input_output()\n",
        "test_dataset = preprocessor.get_train_input_output()\n",
        "\n",
        "train_dataset = data_utils.TensorDataset(train_dataset[0], train_dataset[1])\n",
        "test_dataset = data_utils.TensorDataset(test_dataset[0], test_dataset[1])\n",
        "\n",
        "# get dataloader\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)"
      ],
      "metadata": {
        "id": "Fzbbm_Z1ITXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv9qRJOFr6R-"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, regularizer_type: str, train_loader, test_loader, _lambda=1e-4):\n",
        "    super().__init__()\n",
        "\n",
        "    if regularizer_type == 'l1':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 1\n",
        "\n",
        "    if regularizer_type == 'l2':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 2\n",
        "\n",
        "    if regularizer_type == 'l3':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 3\n",
        "\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "      nn.Linear(8, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.optimizer = optim.Adam(self.mlp.parameters(), lr=0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.mlp(x)\n",
        "    return output\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.mlp[0].weight\n",
        "\n",
        "  def get_loss(self, output, label):\n",
        "    first_layer_weight = self.get_weight()\n",
        "    loss = self.loss(output, label) + self._lambda * torch.norm(first_layer_weight, self.norm)\n",
        "    return loss\n",
        "\n",
        "  def train(self, epochs, save):\n",
        "    self.test_error_list=[]\n",
        "    self.train_error_list=[]\n",
        "\n",
        "    self.mlp.train()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      train_error = 0\n",
        "      for x, gt in self.train_loader:\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self(x)\n",
        "\n",
        "        # train error\n",
        "        train_error += torch.abs(gt-outputs).sum()\n",
        "\n",
        "        loss = self.get_loss(outputs, gt)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      if (epoch+1)%save == 0 or epoch == 0:\n",
        "        self.show_weight_map(epoch, base_dir)\n",
        "\n",
        "      error = float(train_error/len(self.train_loader))\n",
        "      self.train_error_list.append(error)\n",
        "      if (epoch+1)%50 == 0 or epoch == 0:\n",
        "        print(f'\\nTrain Error : {error}')\n",
        "      self.test(epoch)\n",
        "\n",
        "    # # save error\n",
        "    # with open(f'./L{self.norm}_test_error.txt', 'w') as f1:\n",
        "    #   f1.write('\\n'.join(self.test_error_list))\n",
        "    # with open(f'./L{self.norm}_train_error.txt', 'w') as f2:\n",
        "    #   f2.write('\\n'.join(self.train_error_list))\n",
        "\n",
        "  def test(self, epoch):\n",
        "      error=0\n",
        "      self.mlp.eval()\n",
        "      with torch.no_grad():\n",
        "        for x, gt in self.test_loader:\n",
        "          outputs = self(x)\n",
        "          error += torch.abs(gt-outputs).sum()\n",
        "\n",
        "      error = float(error/len(self.test_loader))\n",
        "      self.test_error_list.append(error)\n",
        "      if (epoch+1)%50 == 0 or epoch == 0:\n",
        "        print(f'Test Error : {error}')\n",
        "      self.mlp.train()\n",
        "\n",
        "  def show_weight_map(self, epoch, base_dir):\n",
        "    base_dir = os.path.join(base_dir, f'L{self.norm}')\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    w = self.get_weight()\n",
        "    w = torch.mean(w, dim=0)\n",
        "\n",
        "    # print(f'\\n MAX : {w.max()}, MIN : {w.min()}')\n",
        "    norm_w = torch.abs(w/torch.abs(w).max())\n",
        "    image = np.repeat(norm_w.view(1,8,1).detach().numpy(),3,-1)\n",
        "\n",
        "    # upscale image\n",
        "    scale = 50\n",
        "    new_image = np.zeros((image.shape[0]*scale, image.shape[1]*scale, image.shape[2]))\n",
        "\n",
        "    for i in range(image.shape[0]):\n",
        "      for j in range(image.shape[1]):\n",
        "        new_image[i*scale:(i+1)*scale, j*scale:(j+1)*scale] = image[i,j]\n",
        "\n",
        "    plt.axis('off')\n",
        "    save_path = os.path.join(base_dir, f'weight_{epoch}.png')\n",
        "    plt.imsave(save_path, new_image, cmap='gray')\n",
        "    plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tP70uflwt-yE"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_l1 = MLP(regularizer_type='l1', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l1.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l1_tr_err = model_l1.train_error_list\n",
        "l1_te_err = model_l1.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmFAIk4byLuI"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_l2 = MLP(regularizer_type='l2', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l2.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l2_tr_err = model_l2.train_error_list\n",
        "l2_te_err = model_l2.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRE-bM3gNOvC"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_l3 = MLP(regularizer_type='l3', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l3.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l3_tr_err = model_l3.train_error_list\n",
        "l3_te_err = model_l3.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWHkx4PKX5P0"
      },
      "outputs": [],
      "source": [
        "# plot accuracy progress\n",
        "plt.title(\"Train Error\")\n",
        "plt.plot(range(1,num_epochs+1),l1_tr_err,label=\"l1\")\n",
        "plt.plot(range(1,num_epochs+1),l2_tr_err,label=\"l2\")\n",
        "plt.plot(range(1,num_epochs+1),l3_tr_err,label=\"l3\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy progress\n",
        "plt.title(\"Test Error\")\n",
        "plt.plot(range(1,num_epochs+1),l1_te_err,label=\"l1\")\n",
        "plt.plot(range(1,num_epochs+1),l2_te_err,label=\"l2\")\n",
        "plt.plot(range(1,num_epochs+1),l3_te_err,label=\"l3\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xlabel(\"Test Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rt9FibftaipD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images_from_subdirs(base_dir):\n",
        "    for subdir, _, files in sorted(os.walk(base_dir)):\n",
        "        image_files = sorted([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
        "        image_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "        if image_files:\n",
        "            images = []\n",
        "            for image_file in image_files:\n",
        "                img_path = os.path.join(subdir, image_file)\n",
        "                img = Image.open(img_path)\n",
        "                images.append(img)\n",
        "\n",
        "            num_images = len(images)\n",
        "            fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
        "            plt.suptitle(os.path.basename(subdir), fontsize=16)\n",
        "            if num_images == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for ax, img, file_name in zip(axes, images, image_files):\n",
        "                ax.imshow(img)\n",
        "                ax.set_title(file_name)\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "plot_images_from_subdirs(base_dir)"
      ],
      "metadata": {
        "id": "eXQsIqLnarnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}