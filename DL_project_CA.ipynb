{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howru0321/AIGS538_Deep_Learning/blob/main/DL_project_CA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCXOlGaEpUwy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.data as data_utils\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "\n",
        "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "base_dir = f'CA/{current_time}'\n",
        "\n",
        "num_epochs=1000\n",
        "num_save=100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess():\n",
        "  def __init__(self, train_data, test_data):\n",
        "    self.train_data = train_data\n",
        "    self.test_data = test_data\n",
        "\n",
        "    self.features = train_data.columns\n",
        "    self.in_features = [col for col in train_data.columns if train_data[col].dtype == float and col not in ('median_house_value')]\n",
        "    self.out_features = ['median_house_value']\n",
        "\n",
        "    mean, std = self.get_features_mean_std(train_data)\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def get_features_mean_std(self, data):\n",
        "    return data[self.features].mean(), data[self.features].std()\n",
        "\n",
        "  def get_train_input_output(self):\n",
        "    normalized = (self.train_data[self.features]-self.mean)/self.std\n",
        "    input = torch.Tensor(normalized[self.in_features].values)\n",
        "    output = torch.Tensor(normalized[self.out_features].values)\n",
        "    return [input, output]\n",
        "\n",
        "  def get_test_input_output(self):\n",
        "    normalized = (self.test_data[self.features]-self.mean)/self.std\n",
        "    input = torch.Tensor(normalized[self.in_features].values)\n",
        "    output = torch.Tensor(normalized[self.out_features].values)\n",
        "    return [input, output]\n",
        "\n",
        "# get data\n",
        "train_data = pd.read_csv('./sample_data/california_housing_train.csv')\n",
        "test_data = pd.read_csv('./sample_data/california_housing_test.csv')\n",
        "\n",
        "preprocessor = Preprocess(train_data=train_data, test_data=test_data)\n",
        "train_dataset = preprocessor.get_train_input_output()\n",
        "test_dataset = preprocessor.get_train_input_output()\n",
        "\n",
        "train_dataset = data_utils.TensorDataset(train_dataset[0], train_dataset[1])\n",
        "test_dataset = data_utils.TensorDataset(test_dataset[0], test_dataset[1])\n",
        "\n",
        "# get dataloader\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)"
      ],
      "metadata": {
        "id": "Fzbbm_Z1ITXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv9qRJOFr6R-"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, regularizer_type: str, train_loader, test_loader, _lambda=1e-4):\n",
        "    super().__init__()\n",
        "\n",
        "    if regularizer_type == 'l1':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 1\n",
        "\n",
        "    if regularizer_type == 'l2':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 2\n",
        "\n",
        "    if regularizer_type == 'l3':\n",
        "      self._lambda = _lambda\n",
        "      self.norm = 3\n",
        "\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "\n",
        "    self.mlp = nn.Sequential(\n",
        "      nn.Linear(8, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.optimizer = optim.Adam(self.mlp.parameters(), lr=0.01)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.mlp(x)\n",
        "    return output\n",
        "\n",
        "  def get_weight(self):\n",
        "    return self.mlp[0].weight\n",
        "\n",
        "  def get_loss(self, output, label):\n",
        "    first_layer_weight = self.get_weight()\n",
        "    loss = self.loss(output, label) + self._lambda * torch.norm(first_layer_weight, self.norm)\n",
        "    return loss\n",
        "\n",
        "  def train(self, epochs, save):\n",
        "    self.test_error_list=[]\n",
        "    self.train_error_list=[]\n",
        "\n",
        "    self.mlp.train()\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      train_error = 0\n",
        "      for x, gt in self.train_loader:\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self(x)\n",
        "\n",
        "        # train error\n",
        "        train_error += torch.abs(gt-outputs).sum()\n",
        "\n",
        "        loss = self.get_loss(outputs, gt)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      if (epoch+1)%save == 0 or epoch == 0:\n",
        "        self.show_weight_map(epoch, base_dir)\n",
        "\n",
        "      error = float(train_error/len(self.train_loader))\n",
        "      self.train_error_list.append(error)\n",
        "      if (epoch+1)%50 == 0 or epoch == 0:\n",
        "        print(f'\\nTrain Error : {error}')\n",
        "      self.test(epoch)\n",
        "\n",
        "    # # save error\n",
        "    # with open(f'./L{self.norm}_test_error.txt', 'w') as f1:\n",
        "    #   f1.write('\\n'.join(self.test_error_list))\n",
        "    # with open(f'./L{self.norm}_train_error.txt', 'w') as f2:\n",
        "    #   f2.write('\\n'.join(self.train_error_list))\n",
        "\n",
        "  def test(self, epoch):\n",
        "      error=0\n",
        "      self.mlp.eval()\n",
        "      with torch.no_grad():\n",
        "        for x, gt in self.test_loader:\n",
        "          outputs = self(x)\n",
        "          error += torch.abs(gt-outputs).sum()\n",
        "\n",
        "      error = float(error/len(self.test_loader))\n",
        "      self.test_error_list.append(error)\n",
        "      if (epoch+1)%50 == 0 or epoch == 0:\n",
        "        print(f'Test Error : {error}')\n",
        "      self.mlp.train()\n",
        "\n",
        "  def show_weight_map(self, epoch, base_dir):\n",
        "    base_dir = os.path.join(base_dir, f'L{self.norm}')\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    w = self.get_weight()\n",
        "    w = torch.mean(w, dim=0)\n",
        "\n",
        "    # print(f'\\n MAX : {w.max()}, MIN : {w.min()}')\n",
        "    norm_w = torch.abs(w/torch.abs(w).max())\n",
        "    image = np.repeat(norm_w.view(1,8,1).detach().numpy(),3,-1)\n",
        "\n",
        "    # upscale image\n",
        "    scale = 50\n",
        "    new_image = np.zeros((image.shape[0]*scale, image.shape[1]*scale, image.shape[2]))\n",
        "\n",
        "    for i in range(image.shape[0]):\n",
        "      for j in range(image.shape[1]):\n",
        "        new_image[i*scale:(i+1)*scale, j*scale:(j+1)*scale] = image[i,j]\n",
        "\n",
        "    plt.axis('off')\n",
        "    save_path = os.path.join(base_dir, f'weight_{epoch}.png')\n",
        "    plt.imsave(save_path, new_image, cmap='gray')\n",
        "    plt.imshow(image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tP70uflwt-yE",
        "outputId": "cf84751e-80e4-44f7-cdb0-6fec533d7e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 111.91352844238281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/1000 [00:02<40:38,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 93.66295623779297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 50/1000 [00:37<10:54,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 74.325927734375\n",
            "Test Error : 74.670654296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 100/1000 [01:06<08:16,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 68.89444732666016\n",
            "Test Error : 68.06631469726562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 150/1000 [01:35<07:21,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 66.8887939453125\n",
            "Test Error : 65.27264404296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 199/1000 [02:04<07:45,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 65.87457275390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 200/1000 [02:05<08:56,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 66.58323669433594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 250/1000 [02:34<07:22,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 65.55654907226562\n",
            "Test Error : 64.12936401367188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 300/1000 [03:03<06:03,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 64.2164535522461\n",
            "Test Error : 63.66930389404297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 349/1000 [03:31<05:53,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 62.34126663208008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 350/1000 [03:31<05:50,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 61.092498779296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 400/1000 [04:00<05:06,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 62.75545883178711\n",
            "Test Error : 60.13326644897461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 450/1000 [04:29<06:29,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 61.249568939208984\n",
            "Test Error : 60.04404067993164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 499/1000 [04:56<04:36,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 61.75457763671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 500/1000 [04:57<04:55,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 60.770851135253906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 550/1000 [05:25<03:52,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 61.62607955932617\n",
            "Test Error : 63.09696960449219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 600/1000 [05:53<03:38,  1.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 62.08235168457031\n",
            "Test Error : 61.89715576171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 650/1000 [06:21<03:03,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 62.4543342590332\n",
            "Test Error : 62.80655288696289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 700/1000 [06:51<03:04,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 60.468265533447266\n",
            "Test Error : 60.09061813354492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 750/1000 [07:23<02:36,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 60.26032257080078\n",
            "Test Error : 60.00362777709961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 800/1000 [07:52<01:45,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 60.97529983520508\n",
            "Test Error : 61.71014404296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 849/1000 [08:21<01:42,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 61.44722366333008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 850/1000 [08:22<01:48,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 63.70795822143555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 899/1000 [08:51<00:57,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 59.56570053100586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 900/1000 [08:51<00:56,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 58.87456512451172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 950/1000 [09:20<00:27,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 60.841636657714844\n",
            "Test Error : 60.20439529418945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 999/1000 [09:49<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Error : 60.519775390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [09:50<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error : 59.33164978027344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAABSCAYAAADJltcsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABsElEQVR4nO3YsW2FUBBFQX/LGWWS0QcSlECpRM8lOFxLZybe4IZH+1lrrS8AIOt7egAAMEsMAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIO5nesB/dxzH9IQRz/NMTxixbdv0hBHv+05PGHFd1/SEEfd9T08Yse/79IQR53n+eeMzAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAEPdZa63pEQDAHJ8BAIgTAwAQJwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIj7BSSYFJ1XsBzrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# define model\n",
        "model_l1 = MLP(regularizer_type='l1', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l1.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l1_tr_err = model_l1.train_error_list\n",
        "l1_te_err = model_l1.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmFAIk4byLuI"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_l2 = MLP(regularizer_type='l2', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l2.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l2_tr_err = model_l2.train_error_list\n",
        "l2_te_err = model_l2.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRE-bM3gNOvC"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model_l3 = MLP(regularizer_type='l3', train_loader=train_loader, test_loader=test_loader)\n",
        "# train\n",
        "model_l3.train(epochs=num_epochs, save=num_save)\n",
        "\n",
        "l3_tr_err = model_l3.train_error_list\n",
        "l3_te_err = model_l3.test_error_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWHkx4PKX5P0"
      },
      "outputs": [],
      "source": [
        "# plot accuracy progress\n",
        "plt.title(\"Train Error\")\n",
        "plt.plot(range(1,num_epochs+1),l1_tr_err,label=\"l1\")\n",
        "plt.plot(range(1,num_epochs+1),l2_tr_err,label=\"l2\")\n",
        "plt.plot(range(1,num_epochs+1),l3_tr_err,label=\"l3\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy progress\n",
        "plt.title(\"Test Error\")\n",
        "plt.plot(range(1,num_epochs+1),l1_te_err,label=\"l1\")\n",
        "plt.plot(range(1,num_epochs+1),l2_te_err,label=\"l2\")\n",
        "plt.plot(range(1,num_epochs+1),l3_te_err,label=\"l3\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xlabel(\"Test Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rt9FibftaipD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images_from_subdirs(base_dir):\n",
        "    for subdir, _, files in sorted(os.walk(base_dir)):\n",
        "        image_files = sorted([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
        "        image_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "        if image_files:\n",
        "            images = []\n",
        "            for image_file in image_files:\n",
        "                img_path = os.path.join(subdir, image_file)\n",
        "                img = Image.open(img_path)\n",
        "                images.append(img)\n",
        "\n",
        "            num_images = len(images)\n",
        "            fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
        "            plt.suptitle(os.path.basename(subdir), fontsize=16)\n",
        "            if num_images == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for ax, img, file_name in zip(axes, images, image_files):\n",
        "                ax.imshow(img)\n",
        "                ax.set_title(file_name)\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "plot_images_from_subdirs(base_dir)"
      ],
      "metadata": {
        "id": "eXQsIqLnarnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}